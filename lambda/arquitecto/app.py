import json
import boto3
import os
import uuid
from datetime import datetime
from typing import Dict, List, Any
import logging

# Import MCP Orchestrator
from mcp_orchestrator import MCPOrchestrator

# Configure logging
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# Initialize AWS clients
bedrock_runtime = boto3.client('bedrock-runtime', region_name=os.environ.get('REGION', 'us-east-1'))
dynamodb = boto3.resource('dynamodb', region_name=os.environ.get('REGION', 'us-east-1'))
s3 = boto3.client('s3', region_name=os.environ.get('REGION', 'us-east-1'))

# Get table and bucket names from environment
PROJECTS_TABLE = os.environ.get('PROJECTS_TABLE')
DOCUMENTS_BUCKET = os.environ.get('DOCUMENTS_BUCKET')

projects_table = dynamodb.Table(PROJECTS_TABLE) if PROJECTS_TABLE else None

def lambda_handler(event, context):
    """AWS Lambda handler for arquitecto functionality"""
    try:
        # Handle CORS preflight
        if event.get('httpMethod') == 'OPTIONS':
            return create_response(200, {})
        
        # Parse the request
        if 'body' in event:
            body = json.loads(event['body']) if isinstance(event['body'], str) else event['body']
        else:
            body = event
        
        # Extract parameters
        action = body.get('action', 'chat')
        project_id = body.get('projectId', str(uuid.uuid4()))
        user_id = body.get('userId', 'anonymous')
        
        logger.info(f"ARQUITECTO V3 - Action: {action}, Project: {project_id}")
        
        if action == 'chat':
            return process_arquitecto_chat(body, context)
        else:
            return create_response(400, {'error': 'Invalid action'})
        
    except Exception as e:
        logger.error(f"Error in arquitecto handler: {str(e)}")
        return create_response(500, {
            'error': 'Internal server error',
            'details': str(e)
        })

def process_arquitecto_chat(body: Dict, context) -> Dict:
    """Process chat with arquitecto mode - Amazon Q Developer CLI Style"""
    
    try:
        messages = body.get('messages', [])
        model_id = body.get('modelId', 'anthropic.claude-3-haiku-20240307-v1:0')
        project_id = body.get('projectId', str(uuid.uuid4()))
        user_id = body.get('userId', 'anonymous')
        
        # Handle both projectInfo (Lambda format) and currentProject (Frontend format)
        project_info = body.get('projectInfo', body.get('currentProject', {}))
        project_phase = body.get('projectPhase', 'inicio')
        
        # Ensure project_info is a dict
        if not isinstance(project_info, dict):
            project_info = {}
        
        logger.info(f"ðŸ—ï¸ ARQUITECTO V3 - Processing chat with MCP Orchestrator")
        logger.info(f"Model ID: {model_id}")
        logger.info(f"Messages count: {len(messages)}")
        logger.info(f"Project info: {project_info}")
        logger.info(f"Project phase: {project_phase}")
        logger.info(f"Request body keys: {list(body.keys())}")
        
        if not messages:
            return create_response(400, {'error': 'Messages are required'})
        
        # Initialize MCP Orchestrator with bucket name
        mcp_orchestrator = MCPOrchestrator(DOCUMENTS_BUCKET)
        
        # System prompt for arquitecto mode
        system_prompt = get_arquitecto_system_prompt()
        
        # Prepare the prompt for Bedrock
        prompt_data = prepare_prompt(model_id, system_prompt, messages, project_info, 0)
        
        # Call Bedrock
        response = bedrock_runtime.invoke_model(
            modelId=model_id,
            body=json.dumps(prompt_data)
        )
        
        # Parse response
        response_body = json.loads(response['body'].read())
        
        if 'anthropic' in model_id:
            ai_response = response_body['content'][0]['text']
        else:
            ai_response = response_body.get('generation', response_body.get('outputText', ''))
        
        logger.info(f"AI Response length: {len(ai_response)}")
        
        # ðŸŽ¯ AMAZON Q DEVELOPER CLI STYLE MCP PROCESSING
        mcps_executed = []
        artifacts_generated = []
        
        try:
            logger.info("ðŸš€ Activating MCP Orchestrator...")
            
            # Analyze conversation intent
            intent_analysis = mcp_orchestrator.analyze_conversation_intent(messages, ai_response)
            logger.info(f"Intent analysis: {intent_analysis}")
            
            # Execute MCP workflow based on intent
            project_context = {
                'project_id': project_id,
                'user_id': user_id,
                'project_name': project_info.get('name', ''),
                'project_phase': project_phase,
                **project_info
            }
            
            mcp_results = mcp_orchestrator.execute_mcp_workflow(
                intent_analysis, messages, ai_response, project_context
            )
            
            # Use enhanced response from MCP processing
            if mcp_results.get('success') and mcp_results.get('enhanced_response'):
                ai_response = mcp_results['enhanced_response']
                
            # Update project info with MCP context
            project_info.update(mcp_results.get('context_updates', {}))
            
            # Log MCP execution results
            mcps_executed = mcp_results.get('mcps_executed', [])
            artifacts_generated = mcp_results.get('artifacts_generated', [])
            
            logger.info(f"ðŸ”§ MCPs executed: {mcps_executed}")
            logger.info(f"ðŸ“„ Artifacts generated: {len(artifacts_generated)}")
            
        except Exception as mcp_error:
            logger.error(f"MCP processing error: {str(mcp_error)}")
            # Don't fail the entire request if MCP processing fails
            mcps_executed = []
            artifacts_generated = []
            # Add a note to the response about MCP processing
            ai_response += "\n\nâš ï¸ *Nota: Algunas funciones avanzadas no estÃ¡n disponibles temporalmente.*"
        
        # Create response
        response_data = {
            'response': ai_response,
            'projectId': project_id,
            'userId': user_id,
            'timestamp': datetime.now().isoformat(),
            'model': model_id,
            'mcpUsed': mcps_executed,  # Frontend expects 'mcpUsed'
            'usage': {
                'inputTokens': len(str(messages)),
                'outputTokens': len(ai_response)
            },
            'projectUpdate': project_info,  # Frontend expects 'projectUpdate'
            'projectPhase': project_phase,  # Frontend expects 'projectPhase'
            'mcps_executed': mcps_executed,
            'artifacts_generated': artifacts_generated,
            'project_info': project_info
        }
        
        return create_response(200, response_data)
        
    except Exception as e:
        logger.error(f"Error in process_arquitecto_chat: {str(e)}")
        return create_response(500, {
            'error': 'Error processing chat',
            'details': str(e)
        })

def get_arquitecto_system_prompt() -> str:
    """Get system prompt for arquitecto mode - Amazon Q Developer CLI Style"""
    bucket_name = DOCUMENTS_BUCKET or 'aws-propuestas-v3-documents-prod'
    
    return f"""Actua como Amazon Q Developer CLI para AWS - Arquitecto de Soluciones Senior con 15+ aÃ±os de experiencia.

ðŸŽ¯ MISION: Ser el asistente de arquitectura AWS mÃ¡s inteligente y completo, similar a Amazon Q Developer CLI, con capacidades de:
- AnÃ¡lisis inteligente de requerimientos
- ActivaciÃ³n automÃ¡tica de herramientas especializadas (MCPs)
- GeneraciÃ³n de artefactos profesionales
- GuÃ­a experta en AWS Well-Architected Framework

ðŸ§  INTELIGENCIA ADAPTATIVA:
- Analiza el contexto completo de la conversaciÃ³n
- Detecta automÃ¡ticamente el tipo de proyecto y necesidades
- Activa las herramientas correctas en el momento preciso
- Proporciona respuestas contextuales y personalizadas

ðŸ“‹ FLUJO MAESTRO INTELIGENTE:

1. **ANÃLISIS INICIAL** (Primera interacciÃ³n):
   - Saluda profesionalmente como arquitecto AWS Senior
   - Pregunta: "Â¿CuÃ¡l es el nombre del proyecto que vamos a arquitectar?"
   - Analiza cualquier informaciÃ³n adicional proporcionada

2. **CLASIFICACIÃ“N INTELIGENTE** (Segunda interacciÃ³n):
   - Determina automÃ¡ticamente si es:
     * **Servicio RÃ¡pido**: ImplementaciÃ³n especÃ­fica (EC2, S3, RDS, VPC, etc.)
     * **SoluciÃ³n Integral**: Arquitectura completa (migraciÃ³n, modernizaciÃ³n, nueva aplicaciÃ³n)
   - Pregunta de confirmaciÃ³n si no estÃ¡ claro

3. **RECOPILACIÃ“N ADAPTATIVA**:
   
   **Para Servicios RÃ¡pidos:**
   - Identifica el servicio especÃ­fico requerido
   - Hace SOLO las preguntas esenciales para ese servicio
   - Proporciona configuraciones optimizadas y best practices
   
   **Para Soluciones Integrales:**
   - Entrevista estructurada pero natural
   - Preguntas contextuales basadas en el tipo de soluciÃ³n
   - AnÃ¡lisis de requerimientos tÃ©cnicos y de negocio

4. **GENERACIÃ“N AUTOMÃTICA** (Cuando detecte informaciÃ³n suficiente):
   - Activa automÃ¡ticamente las herramientas especializadas
   - Genera artefactos profesionales completos
   - Proporciona documentaciÃ³n ejecutiva y tÃ©cnica

ðŸ› ï¸ HERRAMIENTAS ESPECIALIZADAS (MCPs) DISPONIBLES:
- **Diagramas**: GeneraciÃ³n automÃ¡tica de arquitecturas visuales
- **DocumentaciÃ³n**: CreaciÃ³n de propuestas ejecutivas y tÃ©cnicas
- **CloudFormation**: Templates de infraestructura como cÃ³digo
- **Costos**: AnÃ¡lisis detallado de precios y optimizaciÃ³n
- **Serverless**: Patrones y mejores prÃ¡cticas Lambda/SAM
- **Frontend**: GuÃ­a para interfaces y experiencia de usuario
- **Seguridad**: AnÃ¡lisis IAM y configuraciones de seguridad

ðŸŽ¨ EXPERIENCIA DE USUARIO:
- ConversaciÃ³n natural y profesional
- Respuestas contextuales y personalizadas
- ActivaciÃ³n transparente de herramientas
- Feedback continuo sobre el progreso
- Entrega de valor en cada interacciÃ³n

ðŸ“Š CALIDAD PROFESIONAL:
- Aplica AWS Well-Architected Framework
- Considera costos, seguridad, escalabilidad y disponibilidad
- Proporciona justificaciÃ³n tÃ©cnica para decisiones
- Genera documentaciÃ³n lista para stakeholders
- Incluye mÃ©tricas y monitoreo recomendados

ðŸ”§ REGLAS DE OPERACIÃ“N:
- Una pregunta clara a la vez
- Si la respuesta es ambigua, solicita clarificaciÃ³n
- NO generes documentos hasta tener informaciÃ³n suficiente
- Activa herramientas automÃ¡ticamente cuando sea apropiado
- MantÃ©n el contexto del proyecto a lo largo de la conversaciÃ³n
- SIN acentos en archivos generados (compatibilidad)

ðŸ’¾ SISTEMA DE ARCHIVOS:
- Bucket S3 del sistema: {bucket_name}
- OrganizaciÃ³n automÃ¡tica por proyecto
- Versionado y backup automÃ¡tico
- Acceso desde secciÃ³n "Proyectos"

ðŸŽ¯ OBJETIVO FINAL:
Proporcionar una experiencia similar a Amazon Q Developer CLI, donde cada interacciÃ³n agrega valor, las herramientas se activan inteligentemente, y el resultado final es una propuesta AWS profesional y completa.

Recuerda: Eres el arquitecto AWS mÃ¡s avanzado disponible. ActÃºa con confianza, precisiÃ³n tÃ©cnica y enfoque en resultados ejecutivos."""

def prepare_prompt(model_id: str, system_prompt: str, messages: List[Dict], 
                  project_info: Dict, current_step: int) -> Dict:
    """Prepare prompt based on model type with correct Bedrock format"""
    
    if 'anthropic' in model_id:
        # Ensure messages have correct format for Anthropic
        formatted_messages = []
        for msg in messages:
            formatted_messages.append({
                "role": msg.get('role', 'user'),
                "content": msg.get('content', '')
            })
        
        return {
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": 4000,
            "temperature": 0.3,
            "system": system_prompt,
            "messages": formatted_messages
        }
    else:
        # For other models like Titan
        conversation = f"System: {system_prompt}\n\n"
        for msg in messages:
            role = msg.get('role', 'user')
            content = msg.get('content', '')
            conversation += f"{role.title()}: {content}\n"
        conversation += "Assistant:"
        
        return {
            "inputText": conversation,
            "textGenerationConfig": {
                "maxTokenCount": 4000,
                "temperature": 0.3,
                "topP": 0.9
            }
        }

def create_response(status_code: int, body: Dict) -> Dict:
    """Create HTTP response with CORS headers"""
    return {
        'statusCode': status_code,
        'headers': {
            'Content-Type': 'application/json',
            'Access-Control-Allow-Origin': '*',
            'Access-Control-Allow-Methods': 'GET, POST, PUT, DELETE, OPTIONS',
            'Access-Control-Allow-Headers': 'Content-Type, Authorization, X-Requested-With'
        },
        'body': json.dumps(body, ensure_ascii=False)
    }
